{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38ea0383-16e9-4a92-a958-a93a72779a56",
   "metadata": {},
   "source": [
    "# What is Docker?\n",
    "Docker is a tool used to automate the deployment of software using lightweight packages called **containers**. Containers are similar to virtual machines: they are isolated from one another and bundle their own software, libraries, and configuration files, but they are more portable and resource friendly.\n",
    "\n",
    "They are used in data engineering to deploy **data pipelines**. A data pipeline is a process that intakes data and does something with the data (processing, cleaning, transforming) and then outputs the data. A pipeline can include many different steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb49061e-c5ff-45ca-b777-c8463198e66d",
   "metadata": {},
   "source": [
    "<div style=\"background: #f8f8f2; text-align: center; border-radius: 6px\">\n",
    "\n",
    "```{mermaid}\n",
    "flowchart LR\n",
    "    A(\"CSV <br>(source)\") ==> B[Data Pipeline]\n",
    "    B[Data Pipeline] ==> C[(\"Postgres table<br>(dest)\")]\n",
    "\n",
    "    style A fill:#f8f8f2,stroke:darkgray,stroke-width:2px\n",
    "    style B fill:#f8f8f2,stroke:darkgray,stroke-width:2px\n",
    "    style C fill:#f8f8f2,stroke:darkgray,stroke-width:2px\n",
    "```\n",
    "\n",
    "</div>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf9bb0c-61ad-4fc4-9a03-bd996850f0df",
   "metadata": {},
   "source": [
    "Advantages of Docker:\n",
    "\n",
    "- Pipelines and analyses are *reproducible*.\n",
    "- Setting up local experiments.\n",
    "- Setting up integration tests (CI/CD)\n",
    "- Easily run in different cloud services."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7744767-1ee5-44e4-b105-783275efc2de",
   "metadata": {},
   "source": [
    "# Installing Docker\n",
    "It's recommended to use a Linux environment in Windows like [WSL](https://learn.microsoft.com/en-us/windows/wsl/install) or MINGW.\n",
    "\n",
    ":::{.callout-tip}\n",
    "To ensure the latest version, [install Docker](https://docs.docker.com/engine/install/ubuntu/) from the official Docker repository.\n",
    ":::\n",
    "\n",
    "First update the `apt` packages index: \n",
    "```default\n",
    "sudo apt update\n",
    "```\n",
    "Then install packages to allow `apt` to use a repository over HTTPS:\n",
    "```default\n",
    "sudo apt install ca-certificates curl gnupg lsb-release\n",
    "```\n",
    "\n",
    "Add Docker's official GPG key (for signing packages):\n",
    "```default\n",
    "sudo mkdir -m 0755 -p /etc/apt/keyrings\n",
    "curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\n",
    "```\n",
    "Setup the repository:\n",
    "```default\n",
    "echo \\\n",
    "  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\\n",
    "  $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\n",
    "```\n",
    "Finally update `apt-get`:\n",
    "```default\n",
    "sudo apt-get update\n",
    "```\n",
    "And install Docker, containerd, and Docker Compose:\n",
    "```default\n",
    "sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n",
    "```\n",
    "If the docker daemon isn't running, restart by:\n",
    "```default\n",
    "sudo service docker start\n",
    "```\n",
    "Verify the installation was successful by running the `hello-world` image:\n",
    "```default\n",
    "sudo docker run hello-world\n",
    "```\n",
    "\n",
    "And congrats -- Docker is now installed!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6244dac0-c9b0-4c5a-b024-b31a4966efdc",
   "metadata": {},
   "source": [
    "# Using Docker\n",
    "A container **image** is a template that contains a set of instructions for creating a container. They are defined in a **Dockerfile** and they are run with `docker run <image name>`.\n",
    "```default\n",
    "docker run hello-world\n",
    "```\n",
    "This will search Docker Hub for an predefined image called `hello-world`, load, and run it. \n",
    "\n",
    "We can also access an Ubuntu terminal within a container:\n",
    "```default\n",
    "docker run -it ubuntu bash\n",
    "```\n",
    "- The `-it` argument will run the prompt in an interactive terminal.\n",
    "- The `bash` argument is a **parameter** passed to the `ubuntu` container to start bash.\n",
    "\n",
    "**Docker containers are stateless** - containers themselves don't save any state (software, packages, libraries, etc.). So if we stop a container with `docker kill <container name>` all installed software and data will be lost.\n",
    "\n",
    "We can run a container with Python pre-installed with:\n",
    "```default\n",
    "docker run -it --entrypoint=bash python:3.9\n",
    "```\n",
    "- The `:3.9` after `python` is a **tag** used to run specific versions of images.\n",
    "- The `--entrypoint=bash` defines the entry point for the container, in this case we want to begin with a `bash` prompt (used to install Python packages).\n",
    "\n",
    "Because containers are stateless we define which software, packages, libraries, etc. we want to begin with by creating a Dockerfile:\n",
    "```dockerfile\n",
    "# this is an example of a simple docker file.\n",
    "\n",
    "FROM python:3.9\n",
    "\n",
    "RUN pip install pandas\n",
    "\n",
    "WORKDIR /app\n",
    "COPY pipeline.py pipeline.py\n",
    "\n",
    "ENTRYPOINT [ \"bash\" ]\n",
    "```\n",
    "- `FROM`: inherit from an existing image.\n",
    "- `RUN`: run commands.\n",
    "- `WORKDIR`: set the working directory.\n",
    "- `COPY`: copy pipeline files from the current directory to the container's working directory.\n",
    "- `ENTRYPOINT`: define a default entry point.\n",
    "\n",
    "Next we need to build the docker image.\n",
    "```default\n",
    "docker build -t test:pandas .\n",
    "```\n",
    "- `test` is the name of our image.\n",
    "- `:pandas` is a tag.\n",
    "- `.` is the directory where the Dockerfile is located.\n",
    "\n",
    "And then use the image to create a container with:\n",
    "```default\n",
    "docker run -it test:pandas\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16857c8e-e520-4c27-ab7d-1d1fb581833f",
   "metadata": {},
   "source": [
    "# Running Postgres in Docker\n",
    "We will use the official Docker Hub image for Postgres:\n",
    "```default\n",
    "docker run -it \\\n",
    "    -e POSTGRES_USER=\"root\" \\\n",
    "    -e POSTGRES_PASSWORD=\"root\" \\\n",
    "    -e POSTGRES_DB=\"ny_taxi\" \\\n",
    "    -v $(pwd)/data:/var/lib/postgresql/data \\\n",
    "    -p 5432:5432 \\\n",
    "postgres:13\n",
    "```\n",
    "- `-e` define an environment variable.\n",
    "- `-v` mount a volume (map a local filesystem with the container filesystem).\n",
    "- `-p` define a port to send and receive database requests.\n",
    "\n",
    "We will access the Postgres database in the container from a python cli in another terminal window (we need a couple packages first):\n",
    "```default\n",
    "pip install pgcli\n",
    "pip install psycopg2\n",
    "pip install psycopg[binary]\n",
    "```\n",
    "Start a connection with the database with:\n",
    "```default\n",
    "pgcli -h localhost -p 5432 -u root -d ny_taxi\n",
    "```\n",
    "- `-h`: URL for the database server.\n",
    "- `-p`: the port.\n",
    "- `-u`: the username.\n",
    "- `-d`: the specific database to connect to.\n",
    "\n",
    "Take a look at the tables with `\\dt`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace23a90-cf84-4485-8803-5a71f750b500",
   "metadata": {},
   "source": [
    "# Loading NYC Taxi Data\n",
    "We will use a python script in Jupyter to load the data. We will need the following modules:\n",
    "```default\n",
    "pip install jupyterlab\n",
    "pip install pandas\n",
    "pip install sqlalchemy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "536818b2-e73e-4daf-8329-2db425c1834f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.3'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c209833-782d-48d6-ac28-d16d3309497b",
   "metadata": {},
   "source": [
    "Find out more about the NYC taxi data at the [NYC Taxi and Limo Commision site](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page). The data was recently converted to parquet files. Download the csv here: [NYC yellow taxi data](https://github.com/DataTalksClub/nyc-tlc-data/releases/tag/yellow).\n",
    "The data dictionary for these files can be found here: [Yellow Trips Data Dictionary](https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e24c832-4059-41fa-ae28-5ea968392fc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 18 columns):\n",
      " #   Column                 Non-Null Count  Dtype         \n",
      "---  ------                 --------------  -----         \n",
      " 0   VendorID               100 non-null    int64         \n",
      " 1   tpep_pickup_datetime   100 non-null    datetime64[ns]\n",
      " 2   tpep_dropoff_datetime  100 non-null    datetime64[ns]\n",
      " 3   passenger_count        100 non-null    int64         \n",
      " 4   trip_distance          100 non-null    float64       \n",
      " 5   RatecodeID             100 non-null    int64         \n",
      " 6   store_and_fwd_flag     100 non-null    object        \n",
      " 7   PULocationID           100 non-null    int64         \n",
      " 8   DOLocationID           100 non-null    int64         \n",
      " 9   payment_type           100 non-null    int64         \n",
      " 10  fare_amount            100 non-null    float64       \n",
      " 11  extra                  100 non-null    float64       \n",
      " 12  mta_tax                100 non-null    float64       \n",
      " 13  tip_amount             100 non-null    float64       \n",
      " 14  tolls_amount           100 non-null    float64       \n",
      " 15  improvement_surcharge  100 non-null    float64       \n",
      " 16  total_amount           100 non-null    float64       \n",
      " 17  congestion_surcharge   100 non-null    float64       \n",
      "dtypes: datetime64[ns](2), float64(9), int64(6), object(1)\n",
      "memory usage: 14.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    '../data/yellow_tripdata_2021-01.csv',\n",
    "    nrows=100,\n",
    "    parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime']\n",
    ")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4850d7e3-5f42-45e4-923e-7a52c2fc49ad",
   "metadata": {},
   "source": [
    "To load the data into Postgres, we first need to generate a schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0fd8423-6b22-4135-b51e-065e063468c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.base.Connection at 0x227bd8f2c90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\n",
    "engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e15bbab6-d8c7-4bcd-8e6c-396783fab177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE yellow_taxi_data (\n",
      "\t\"VendorID\" BIGINT, \n",
      "\ttpep_pickup_datetime TIMESTAMP WITHOUT TIME ZONE, \n",
      "\ttpep_dropoff_datetime TIMESTAMP WITHOUT TIME ZONE, \n",
      "\tpassenger_count BIGINT, \n",
      "\ttrip_distance FLOAT(53), \n",
      "\t\"RatecodeID\" BIGINT, \n",
      "\tstore_and_fwd_flag TEXT, \n",
      "\t\"PULocationID\" BIGINT, \n",
      "\t\"DOLocationID\" BIGINT, \n",
      "\tpayment_type BIGINT, \n",
      "\tfare_amount FLOAT(53), \n",
      "\textra FLOAT(53), \n",
      "\tmta_tax FLOAT(53), \n",
      "\ttip_amount FLOAT(53), \n",
      "\ttolls_amount FLOAT(53), \n",
      "\timprovement_surcharge FLOAT(53), \n",
      "\ttotal_amount FLOAT(53), \n",
      "\tcongestion_surcharge FLOAT(53)\n",
      ")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pd.io.sql.get_schema(df, name='yellow_taxi_data', con=engine))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87941759-e631-426d-b5d0-77ba9030dbc4",
   "metadata": {},
   "source": [
    "When reading in large amounts of data into a database, it's a good idea to break the insert up into chunks and read those in one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f4b62081-9d1a-4fb7-b7d2-f2e0a6f0088f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iter = pd.read_csv(\n",
    "    '../data/yellow_tripdata_2021-01.csv',\n",
    "    parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'],\n",
    "    iterator=True,\n",
    "    chunksize=100000,\n",
    "    low_memory=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bfc9299-468d-4e45-834f-97e83a8f0599",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = next(df_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48656f5-108e-4e97-bde7-913ea4d77f34",
   "metadata": {},
   "source": [
    "Let's insert the header row of the dataframe to create the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82936446-5f6a-4b2d-8548-c80f00470b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(n=0).to_sql(name='yellow_taxi_data', con=engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97288a2c-747b-4632-8ee9-26e7a1e493fa",
   "metadata": {},
   "source": [
    "Check that the table was created with `\\d yellow_taxi_data`. Now let's read the first chunk of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c8cf9d4-bd8a-4c8b-b140-6a5722e415cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_sql(name='yellow_taxi_data', con=engine, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a761b97c-10cd-467a-a1c1-02025e9c2845",
   "metadata": {},
   "source": [
    "Check that the first chunk was inserted with `SELECT * FROM yellow_taxi_data`. Now let's read in the rest of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37e60524-721d-4520-bb08-5c63a71bfdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insert another chunk..., took 0.478 seconds.\n",
      "insert another chunk..., took 0.636 seconds.\n",
      "insert another chunk..., took 1.045 seconds.\n",
      "insert another chunk..., took 0.601 seconds.\n",
      "insert another chunk..., took 1.093 seconds.\n",
      "insert another chunk..., took 0.820 seconds.\n",
      "insert another chunk..., took 0.862 seconds.\n",
      "insert another chunk..., took 0.642 seconds.\n",
      "insert another chunk..., took 0.640 seconds.\n",
      "insert another chunk..., took 1.017 seconds.\n",
      "insert another chunk..., took 0.635 seconds.\n",
      "insert another chunk..., took 0.884 seconds.\n",
      "insert another chunk..., took 0.740 seconds.\n",
      "insert another chunk..., took 0.520 seconds.\n",
      "Reached the end of the iterator.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "try:\n",
    "    while True:\n",
    "        t_start = time()\n",
    "\n",
    "        df = next(df_iter)\n",
    "        df.to_sql(name='yellow_taxi_data', con=engine, if_exists='append')\n",
    "\n",
    "        t_end = time()\n",
    "        print(f'insert another chunk..., took {t_end - t_start:.3f} seconds.')\n",
    "except StopIteration:\n",
    "    print(\"Reached the end of the iterator.\")\n",
    "finally:\n",
    "    del df_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb53d426-cada-48c0-b4cc-0af0f8b01b06",
   "metadata": {},
   "source": [
    "# Using pgAdmin and Docker Networks\n",
    "\n",
    "**pgAdmin** is a web-based GUI tool used to interact with the Postgres database sessions. We can run it in a separate container:\n",
    "```default\n",
    "docker run -it \\\n",
    "    -e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n",
    "    -e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\n",
    "    -p 8080:80 \\\n",
    "    dpage/pgadmin4\n",
    "```\n",
    "\n",
    "To list the currently running containers we can use `docker ps`. We should have two containers running:\n",
    "```default\n",
    "docker ps\n",
    "```\n",
    "\n",
    "However, in order for the pgAdmin client to communicate with our Postgres server, we need to setup a [Docker network](https://docs.docker.com/engine/reference/commandline/network/) so the containers can communicate with each other.\n",
    "\n",
    "First we need to create a docker network:\n",
    "```default\n",
    "docker network create pg-network\n",
    "```\n",
    "\n",
    "Next we need to stop our previously running containers:\n",
    "```default\n",
    "docker kill pgdatabase\n",
    "docker kill dpage/pgadmin4\n",
    "```\n",
    "and restart them with the `--network` commands:\n",
    "```default\n",
    "docker run -it \\\n",
    "    -e POSTGRES_USER=\"root\" \\\n",
    "    -e POSTGRES_PASSWORD=\"root\" \\\n",
    "    -e POSTGRES_DB=\"ny_taxi\" \\\n",
    "    -v $(pwd)/data:/var/lib/postgresql/data \\\n",
    "    -p 5432:5432 \\\n",
    "    --network=pg-network \\\n",
    "    --name pg-database \\\n",
    "postgres:13\n",
    "```\n",
    "```default\n",
    "docker run -it \\\n",
    "    -e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n",
    "    -e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\n",
    "    -p 8080:80 \\\n",
    "    --network=pg-network\n",
    "    --name pgadmin\n",
    "    dpage/pgadmin4\n",
    "```\n",
    "\n",
    "We can view the currently running networks with:\n",
    "```default\n",
    "docker network ls\n",
    "```\n",
    "\n",
    "Now navigate to pgAdmin at [localhost:8080](localhost:8080) and login with our set email and password.\n",
    "\n",
    "Next we will create a server in pgAdmin: `Servers` > `Create` > `Server...` and fill in the fields:\n",
    "\n",
    "* Host name/address: `pgdatabase`\n",
    "* Port: `5432`\n",
    "* Maintenance database: `postgres`\n",
    "* Username: `root`\n",
    "* Password: `root`\n",
    " \n",
    "Now we are connected to our Postgres database and can make queries!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c4bd47-427b-40ac-b6d9-9c416aa98e04",
   "metadata": {},
   "source": [
    "# Dockerizing the data ingestion script\n",
    "In order to run the NYC taxi data ingestion script inside a Docker container we need to convert the Jupyter notebook code into a script named `ingest_data.py`:\n",
    "\n",
    "```python\n",
    "from time import time\n",
    "import argparse\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def main(params):\n",
    "    \"\"\" Download and read NYC Yellow Taxi Data into a Postges table.\"\"\"\n",
    "    user = params.user\n",
    "    password = params.password\n",
    "    host = params.host\n",
    "    port = params.port\n",
    "    db = params.db\n",
    "    table_name = params.table_name\n",
    "    url = params.url\n",
    "    csv_name = \"output.csv.gz\"\n",
    "    \n",
    "    # download the csv\n",
    "    os.system(f\"wget {url} -O {csv_name}\")\n",
    "    \n",
    "    # setup data and table for ingestion\n",
    "    engine = create_engine(f'postgresql://{user}:{password}@{host}:{port}/{db}')\n",
    "    \n",
    "    df_iter = pd.read_csv(\n",
    "        '../data/yellow_tripdata_2021-01.csv',\n",
    "        parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'],\n",
    "        iterator=True,\n",
    "        chunksize=100000,\n",
    "        low_memory=False\n",
    "    )\n",
    "    df = next(df_iter)\n",
    "    \n",
    "    df.head(n=0).to_sql(name='yellow_taxi_data', con=engine, if_exists='replace')\n",
    "    df.to_sql(name='yellow_taxi_data', con=engine, if_exists='append')\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            t_start = time()\n",
    "\n",
    "            df = next(df_iter)\n",
    "            # df.to_sql(name='yellow_taxi_data', con=engine, if_exists='append')\n",
    "\n",
    "            t_end = time()\n",
    "            print(f'insert another chunk..., took {t_end - t_start:.3f} seconds.')\n",
    "    except StopIteration:\n",
    "        print(\"Reached the end of the iterator.\")\n",
    "    finally:\n",
    "        del df_iter\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # set up argparse arguments\n",
    "    parser = argparse.ArgumentParser(description=\"Ingest csv data to Postgres\")\n",
    "    parser.add_argument('user', help='user name for postgres')\n",
    "    parser.add_argument('pass', help='password for postgres')\n",
    "    parser.add_argument('host', help='host for postgres')\n",
    "    parser.add_argument('port', help='port for postgres')\n",
    "    parser.add_argument('db', help='database name for postgres')\n",
    "    parser.add_argument('table_name', help='table to write results to')\n",
    "    parser.add_argument('url', help='url of the csv file')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    main(args)\n",
    "```\n",
    "\n",
    "Now to run this script in a container, let's:\n",
    "\n",
    "1. Write the Dockerfile\n",
    "```Dockerfile\n",
    "FROM python:3.11\n",
    "\n",
    "RUN apt-get install wget\n",
    "RUN pip install pandas sqlalchemy psycopg2\n",
    "\n",
    "WORKDIR /app\n",
    "COPY ingest_data.py ingest_data.py\n",
    "\n",
    "ENTRYPOINT [ \"python\", \"ingest_data.py\" ]\n",
    "```\n",
    "\n",
    "2. Build the Dockerfile\n",
    "```default\n",
    "docker build -t taxi_ingest:v001 .\n",
    "```\n",
    "\n",
    "3. Run the Docker image\n",
    "```default\n",
    "URL=\"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz\"\n",
    "\n",
    "docker run -it \\\n",
    "  --network=pg-network \\\n",
    "  taxi_ingest:v001 \\\n",
    "    --user=root \\\n",
    "    --password=root \\\n",
    "    --host=pgdatabase \\\n",
    "    --port=5432 \\\n",
    "    --db=ny_taxi \\\n",
    "    --table_name=yellow_taxi_trips \\\n",
    "    --url=${URL}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51cf475-b621-4ae1-b352-3937a5dc65b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Using the Docker CLI\n",
    "The [Docker CLI](https://docs.docker.com/engine/reference/run/) has several useful commands for managing containers, networks, and volumes.\n",
    "\n",
    ":::{.callout-note}\n",
    "If you receive the error: `Cannot connect to the Docker daemon`, try restarting the docker service with:\n",
    "```default\n",
    "sudo service docker start\n",
    "```\n",
    ":::\n",
    "\n",
    "List containers\n",
    "``` default\n",
    "docker ps -a --format \"table {{.ID}}\\t{{.Image}}\\t{{.Name}}\"\n",
    "```\n",
    "- `-a`: list all containers, omit to list only running containers.\n",
    "- `--format`: format the table output with {{.ColumnName}} in quotes.\n",
    "Enter a container (start an interactive shell within the container)\n",
    "``` default\n",
    "docker exec -it CONTAINER bash\n",
    "```\n",
    "Stop (or kill) a running container\n",
    "```default\n",
    "docker stop CONTAINER\n",
    "docker kill CONTAINER\n",
    "```\n",
    "Remove containers\n",
    "```\n",
    "docker rm CONTAINER\n",
    "```\n",
    "Remove all containers\n",
    "```\n",
    "docker rm -f $(docker ps -aq)\n",
    "```\n",
    "Create networks\n",
    "```default\n",
    "docker network create NETWORK\n",
    "```\n",
    "List networks\n",
    "```default\n",
    "docker network ls\n",
    "```\n",
    "Connection (disconnect) networks from specific containers\n",
    "```default\n",
    "docker network connect NETWORK CONTAINER\n",
    "docker network disconnect NETWORK CONTAINER\n",
    "```\n",
    "Remove networks\n",
    "```default\n",
    "docker network rm NETWORK\n",
    "```\n",
    "Remove all unused networks (networks that are not connected to a container)\n",
    "```default\n",
    "docker network prune\n",
    "```\n",
    "Create volumes\n",
    "```default\n",
    "docker volume create VOLUME\n",
    "```\n",
    "Display information about the volume\n",
    "```default\n",
    "docker volume inspect VOLUME\n",
    "```\n",
    "List volumes\n",
    "```default\n",
    "docker volume ls\n",
    "```\n",
    "Remove volumes\n",
    "```default\n",
    "docker volume rm VOLUME\n",
    "```\n",
    "Remove all unused volumes (volumes that are not connected to a container)\n",
    "```default\n",
    "docker volume prune\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e62330-6962-442f-be3d-ed761e7d7089",
   "metadata": {},
   "source": [
    "# Docker Compose\n",
    "[Docker Compose](https://docs.docker.com/compose/compose-file/compose-file-v2/) is a tool to configure and run multiple docker containers (within an automatically created docker network) in one convenient `docker-compose.yaml` file. We just need to list our containers and image, environment, volume, and port parameters:\n",
    "```yaml\n",
    "services:\n",
    "  pgdatabase:\n",
    "    image: postgres:13\n",
    "    environment:\n",
    "      - POSTGRES_USER=root\n",
    "      - POSTGRES_PASSWORD=root\n",
    "      - POSTGRES_DB=ny_taxi\n",
    "    volumes:\n",
    "      - postgres_data:/var/lib/postgresql/data:rw\n",
    "    ports:\n",
    "      - \"5432:5432\"\n",
    "    networks:\n",
    "      - pg-network\n",
    "\n",
    "  pgadmin:\n",
    "    image: dpage/pgadmin4\n",
    "    environment:\n",
    "      - PGADMIN_DEFAULT_EMAIL=admin@admin.com\n",
    "      - PGADMIN_DEFAULT_PASSWORD=root\n",
    "    volumes:\n",
    "      - type: volume\n",
    "        source: pgadmin_data\n",
    "        target: /var/lib/pgadmin\n",
    "    ports:\n",
    "      - \"8080:80\"\n",
    "    networks:\n",
    "      - pg-network\n",
    "  \n",
    "  taxi_ingest:\n",
    "    container_name: taxi-data-ingest\n",
    "    image: taxi_ingest:v001\n",
    "    build:\n",
    "      context: .\n",
    "      dockerfile: nyc_ingest.Dockerfile\n",
    "    command:\n",
    "      - --user=root\n",
    "      - --password=root\n",
    "      - --host=pgdatabase\n",
    "      - --port=5432\n",
    "      - --db=ny_taxi\n",
    "      - --table_name=yellow_taxi_trips\n",
    "      - --url=https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz\n",
    "    networks:\n",
    "      - pg-network\n",
    "    depends_on:\n",
    "      - pgdatabase\n",
    "\n",
    "networks:\n",
    "  pg-network:\n",
    "    name: pg-network\n",
    "\n",
    "volumes:\n",
    "  postgres_data:\n",
    "  pgadmin_data:\n",
    "\n",
    "``` \n",
    "\n",
    "Now run the compose file with:\n",
    "```default\n",
    "docker compose up -d --build\n",
    "```\n",
    "- `-d`: runs the containers in detached mode in order to continue to use the current terminal.\n",
    "- `--build`: specifically builds the image (only needed the first time).\n",
    "\n",
    "And shutdown the containers with:\n",
    "```default\n",
    "docker compose down\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed2b9dc-7de5-4e98-9f68-8650693ac9fd",
   "metadata": {},
   "source": [
    "# What is Terraform?\n",
    "Terraform is an open source IAC (Infrastructure As Code) tool by Hashicorp that allows you to provision resources with decorated configuration files.\n",
    "\n",
    "This lets you manage the infrastructure lifecycle in a set of configuration files that can be version controlled, reused, and shared.\n",
    "\n",
    "This also lets us track resource changes accross data pipelines.\n",
    "\n",
    "Install Terraform (Windows) from [terraform.io/downloads](https://developer.hashicorp.com/terraform/downloads).\n",
    "\n",
    "Install Terraform (Linux) with:\n",
    "```default\n",
    "wget -O- https://apt.releases.hashicorp.com/gpg | gpg --dearmor | sudo tee /usr/share/keyrings/hashicorp-archive-keyring.gpg\n",
    "echo \"deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/hashicorp.list\n",
    "sudo apt update && sudo apt install terraform\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35558655-9cb8-4896-9eeb-de7f08da11df",
   "metadata": {},
   "source": [
    "# Google Cloud Platform Setup\n",
    "[Google Cloud Platform (GCP)](https://console.cloud.google.com/) works in **projects**. We need to create a new project.\n",
    "\n",
    "1. Create New Project:\n",
    "    - Project Name: \"dtc-de-2023\".\n",
    "    - Project ID: generate, needs to be unique across GCP.\n",
    "    \n",
    "2. Create a Service Account\n",
    "    - Navigate: `IAM & Admin` > `Service Accounts`\n",
    "    - Service account name: \"dtc-de-2023-user\".\n",
    "    - Service account ID: does not need to be unique.\n",
    "    - Service account description: \"DTC DE course\".\n",
    "    - Grant this role \"Viewer\" status for now.\n",
    "\n",
    ":::{.callout-note}\n",
    "A **service account** provides a set of credentials tied to a particular service or server. This allows services to interact with one another without the user or admin account and provides for security or permissions to be setup for each service (or set of services).\n",
    ":::\n",
    "\n",
    "3. Generate a key\n",
    "    - Navigate to Manage Keys > Add Key > Create New Key.\n",
    "    - Set key type to JSON.\n",
    "    \n",
    "4. Download GCP SDK\n",
    "    - [Install GCP SDK](https://cloud.google.com/sdk/docs/install-sdk) and check installation with `gcloud -v`.\n",
    "    - Set environment variable for GCP credentials:\n",
    "    ```default\n",
    "    set GOOGLE_APPLICATION_CREDENTIAL=\"<path>/<to>/<service-account-authkey>.json\"\n",
    "    ```\n",
    "    - Refresh token, and verify authentication:\n",
    "    ```default\n",
    "    gcloud auth application-default login\n",
    "    ```\n",
    "\n",
    "5. Create GCP Resources\n",
    "\n",
    "We will create 2 GCP resources in the Google Cloud web environment:\n",
    "\n",
    "- **Google Cloud Storage (GCS)**: Data Lake (a \"bucket\" to store raw data in a directory of flat files (.csv, .parquet, etc.)\n",
    "- **Big Query**: Data Warehouse (data is modeled into fact and dimension tables for querying)\n",
    "\n",
    "But first we need to set up permissions for our service account:\n",
    "\n",
    "- Navigate to IAM, select your service account and click **Edit Principal**\n",
    "- We will add the following roles:\n",
    "    - Create a \"Storage Admin\" role (allows us to create buckets).\n",
    "    - Create a \"Storage Object Admin\" role (allows us to dump files).\n",
    "    - Create a \"Big Query Admin\" role (for querying).\n",
    "    \n",
    "6. Enable APIs\n",
    "\n",
    "APIs enable communication between our local environment and the cloud. We need to enable the following APIs:\n",
    "\n",
    "- [Identity and Access Management (IAM) API](https://console.cloud.google.com/apis/library/iam.googleapis.com)\n",
    "- [ IAM Service Account Credentials API](https://console.cloud.google.com/apis/library/iamcredentials.googleapis.com)\n",
    "    \n",
    ":::{.callout-note}\n",
    "In a production environment you would create custom roles to associate permissions on a per resource basis. For example, you would create one service account for Terraform with all of the \"admin\" roles and then a separate account for each data pipeline with separate permissions.\n",
    ":::\n",
    "\n",
    "We are now ready to go - refresh your service-accounts auth-token if needed:\n",
    "```default\n",
    "gcloud auth application-default login\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b5283f-b9f4-46a7-b285-27b66b92b777",
   "metadata": {},
   "source": [
    "# Terraform Setup\n",
    "Terraform files (.tf) are written in the Hashicorp Configuration Language.\n",
    "\n",
    "Create 3 files:\n",
    "\n",
    "- `.terraform-version`: a simple file indicating the Terraform version.\n",
    "- `main.tf`: the main configuration file (filename can be anything, convention is \"main\").\n",
    "- `variables.tf`: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0dc1b4-d33b-43d7-a952-585c99f5d149",
   "metadata": {},
   "source": [
    "Example .tf file:\n",
    "```default\n",
    "terraform {\n",
    "    required_version = \">= 1.0\"\n",
    "    backend \"local\" {} # can change from \"local\" to \"gcs\" or \"s3\" depending on your provider.\n",
    "    providers {\n",
    "        google = {\n",
    "            source = \"hasicorp/google\" # declaring a predefined public configuration for this service provider.\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "## Providers\n",
    "Terraform works with plugins called **providers** for each data source. These add a set of predefined resources and data types that Terraform can manage. The Terraform Registry is the main directory of publicly available providers for most major cloud infrastructure platforms. These are declared with the `provider` tag:\n",
    "```default\n",
    "provider \"google\" {\n",
    "    project = var.project\n",
    "    region = var.region\n",
    "    // credentails = file(var.credentials) # instead of setting env variables.\n",
    "}\n",
    "```\n",
    "\n",
    "## Variables\n",
    "Notice certain variables in `main.tf` are preceded with `var.`. These values come from the `variables.tf` file. We begin the `variables.tf` file with declaring `local` variables:\n",
    "```terraform\n",
    "locals {\n",
    "    data_lake_bucket = \"dtc_data_lake\"\n",
    "}\n",
    "```\n",
    "Variables are generally passed during runtime. Default variables are *optional* runtime arguments, defined variables are *mandatory* runtime arguments. \n",
    "```default\n",
    "# this variable is mandatory and it's value will be entered at runtime\n",
    "variable \"project\" {\n",
    "    description = \"GCP Project ID\" \n",
    "}\n",
    "\n",
    "# this variable is optional as it has a default \"us-west1\"\n",
    "variable \"region\" {\n",
    "    description = \"Region for GCP resources.\"\n",
    "    default = \"us-west1\"\n",
    "    type = string\n",
    "}\n",
    "```\n",
    "\n",
    "## Execution\n",
    "- `terraform init`: Initialize and install.\n",
    "- `terraform plan`: Describes the actions that Terraform will take to match changes against the previous state.\n",
    "- `terraform apply`: Apply changes to the cloud: create or update new resources, increase memory, etc.\n",
    "- `terraform destroy`: Remove your stack and resources from the cloud (used to save on idle resources).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c461160-da16-4248-aff1-ac9c8580fd83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
